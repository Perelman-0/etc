{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams['figure.figsize'] = (12.0, 9.0)\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Generate Data\n",
    "x, y = make_regression(n_samples=1000, n_features=1, n_targets=1, noise=5.0, random_state=42)\n",
    "data = pd.DataFrame({\"x\" : x.reshape(-1, ), \"y\" : y})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data visualization\n",
    "sns.scatterplot(data=data, x='x', y='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Gradient Descent\n",
    "\n",
    "def gradient_descent(w=0.1, b=0.1, learning_rate=1e-2, max_iter=100, tol=1e-4):\n",
    "\n",
    "    '''\n",
    "        - 2차원에서의 단변량 Linear Regression 모델의 gradient descent algorithm을 구현합니다.\n",
    "        - y = wx + b로 정의되며, learning_rate를 입력받아 epochs 만큼 weight update를 진행합니다.\n",
    "        - max_iter만큼 진행하다가 주어진 max_iter전에 수렴한 것 같다면 멈춰도 됩니다.\n",
    "        - 수렴 조건은 업데이트 되는 loss가 tol 이하일 때를 의미합니다.\n",
    "    '''\n",
    "\n",
    "    ## TO-DO ##\n",
    "    # gradient descent algorithm 구현하기\n",
    "\n",
    "\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Result\n",
    "\n",
    "w =\n",
    "b =\n",
    "\n",
    "sns.scatterplot(data=data, x='x', y='y')\n",
    "sns.lineplot(x=data['x'], y=w*data['x'] + b)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
